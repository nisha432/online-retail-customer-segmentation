{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "7hBIi_osiCS2",
        "u3PMJOP6ngxN",
        "bKJF3rekwFvQ",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "7wuGOrhz0itI",
        "578E2V7j08f6",
        "yiiVWRdJDDil",
        "T5CmagL3EC8N",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nisha432/online-retail-customer-segmentation/blob/main/ONLINE_RETAIL_CUSTOMER_SEGMENTAION.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - \n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Unsupervised\n",
        "##### **Contribution**    - Individual\n",
        "##### **Team Member 1 -** NISHA AHIRE \n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the summary here within 500-600 words."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The project \"Online Retail Customer Segmentation\" focuses on analyzing customer data from an online retail business to identify different customer segments. The dataset includes information such as invoice details, product descriptions, quantities, prices, customer IDs, countries, and more.\n",
        "\n",
        "The analysis begins with data cleaning and preprocessing steps to handle missing values, remove duplicates, and convert data types if necessary. Exploratory data analysis (EDA) techniques are then applied to gain insights into customer behavior, purchase patterns, and other relevant factors.\n",
        "\n",
        "Some of the key EDA questions addressed in the project include:\n",
        "\n",
        "Distribution of Quantity: Examining the distribution of the quantity of items ordered helps identify any outliers or unusual patterns in customer purchasing behavior.\n",
        "\n",
        "Prevalence of Discounts by Month: Analyzing the percentage of invoices with discounts over different months provides insights into the fluctuation of discount usage and its impact on customer behavior.\n",
        "\n",
        "Average Purchase Frequency by Cohort Month-Year: Calculating the average purchase frequency based on cohort month-year helps identify cohorts with higher or lower purchase frequency, enabling targeted marketing strategies.\n",
        "\n",
        "Impact of Discounts on Average InvoiceTotal: Hypothesis testing is conducted to determine if there is a significant difference in the average InvoiceTotal between customers who received a discount and those who did not.\n",
        "\n",
        "Identification of High-Priced Items: Analyzing the average unit prices of products helps identify specific stock codes or descriptions associated with higher unit prices. The contribution of these high-priced items to overall revenue is also assessed.\n",
        "\n",
        "\n",
        "The project also explores other aspects such as outlier treatment techniques, visualizations (e.g., box plots), and text data processing (e.g., lowercasing and contraction handling). Additionally, hypothesis testing is performed to compare different customer segments and assess significant differences in key metrics.\n",
        "\n",
        "Overall, the project aims to provide valuable insights into customer segmentation, purchase behavior, and factors influencing business growth. These insights can be leveraged to develop targeted marketing strategies, optimize pricing strategies, and enhance overall business performance in the online retail industry.\n",
        "\n"
      ],
      "metadata": {
        "id": "qYX15zLljlU4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## https://github.com/nisha432/online-retail-customer-segmentation/blob/main/ONLINE_RETAIL_CUSTOMER_SEGMENTAION.ipynb"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The objective of this project is to perform unsupervised customer segmentation for an online retail business. By analyzing customer behavior and characteristics, the aim is to identify distinct customer segments that can be targeted with personalized marketing strategies. The segmentation will be based on various features such as purchase history, demographics, and customer activity ."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required. \n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits. \n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule. \n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Import Libraries\n",
        "# Import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import datetime as dt\n",
        "import missingno as msno\n",
        "import matplotlib.cm as cm\n",
        "import missingno as msno\n",
        "from os import path\n",
        "from PIL import Image\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "\n",
        "#for nlp\n",
        "from sklearn import preprocessing\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_samples\n",
        "import scipy.cluster.hierarchy as sch\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "!pip install kaleido\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7aa28a1-9a74-4f05-b410-c4f0df93c0be"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kaleido in /usr/local/lib/python3.10/dist-packages (0.2.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "q8ro8_ABQ-AT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "552e5b5d-c2ad-4a99-fa1c-b032321b833a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df=pd.read_excel(\"/content/drive/MyDrive/AlmaBetter/datasets/Online Retail.xlsx\")\n"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "# Find duplicate rows based on all columns\n",
        "duplicates = df[df.duplicated()]\n",
        "# Print the duplicate rows\n",
        "print(duplicates)"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def resumetable(df):\n",
        "    print(f\"Dataset Shape: {df.shape}\")\n",
        "    summary = pd.DataFrame(df.dtypes,columns=['dtypes'])\n",
        "    summary = summary.reset_index()\n",
        "    summary['Name'] = summary['index']\n",
        "    summary = summary[['Name','dtypes']]\n",
        "    summary['Missing'] = df.isnull().sum().values    \n",
        "    summary['Uniques'] = df.nunique().values\n",
        "    summary['First Value'] = df.loc[0].values\n",
        "    summary['Second Value'] = df.loc[1].values\n",
        "    return summary\n",
        "result = resumetable(df)\n",
        "result.sort_values('Missing', ascending= False)"
      ],
      "metadata": {
        "id": "ol-fLhUEUnTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Missing Values/Null Values Count\n",
        " print(df.isnull().sum())"
      ],
      "metadata": {
        "id": "8N_ABg9ubT3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import signal\n",
        "\n",
        "# Define a timeout function\n",
        "def timeout_handler(signum, frame):\n",
        "    raise TimeoutError(\"Operation timed out\")\n",
        "\n",
        "# Set a timeout value (in seconds)\n",
        "timeout_seconds = 10\n",
        "\n",
        "# Set the timeout signal\n",
        "signal.signal(signal.SIGALRM, timeout_handler)\n",
        "signal.alarm(timeout_seconds)\n",
        "\n",
        "# Perform the operation that may take too long\n",
        "sns.heatmap(df.isna().transpose(),\n",
        "            cmap=\"YlGnBu\",\n",
        "            cbar_kws={'label': 'Missing Data'})\n",
        "plt.savefig(\"visualizing_missing_data_with_heatmap_Seaborn_Python.png\", dpi=100)\n",
        "\n",
        "# Reset the alarm\n",
        "signal.alarm(0)\n",
        "\n"
      ],
      "metadata": {
        "id": "bNW10tvqbfYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "sns.displot(\n",
        "    data=df.isna().melt(value_name=\"missing\"),\n",
        "    y=\"variable\",\n",
        "    hue=\"missing\",\n",
        "    multiple=\"fill\",\n",
        "    aspect=1.25\n",
        ")\n",
        "plt.savefig(\"visualizing_missing_data_with_barplot_Seaborn_distplot.png\", dpi=100)"
      ],
      "metadata": {
        "id": "ZtMFnsKbbzdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The given dataset has 541909 rows and 8 columns,and one columns had missing values.The following column had missing valuews-customerID \n",
        "\n"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "y_RlydkC7KH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "hf-ZksV17NQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description "
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. InvoiceNo: Invoice number. Nominal, a 6-digit integral number uniquely assigned to each transaction. If this code starts with letter 'c', it indicates a cancellation.\n",
        "\n",
        "2. StockCode: Product (item) code. Nominal, a 5-digit integral number uniquely assigned to each distinct product.\n",
        "\n",
        "3. Description: Product (item) name. Nominal.\n",
        "\n",
        "4. Quantity: The quantities of each product (item) per transaction. \n",
        "\n",
        "\n",
        "5. InvoiceDate: Invice Date and time. Numeric, the day and time when each transaction was generated.\n",
        "\n",
        "\n",
        "6. UnitPrice: Unit price. Numeric, Product price per unit in sterling.\n",
        "\n",
        "\n",
        "7. CustomerID: Customer number. Nominal, a 5-digit integral number uniquely assigned to each customer.\n",
        "\n",
        "8. Country: Country name. Nominal, the name of the country where each customer resides."
      ],
      "metadata": {
        "id": "e2lKHDS0eWko"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "for column in df.columns:\n",
        "    unique_values = df[column].unique()\n",
        "    print(f\"Unique values in {column} column: {unique_values}\")"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the number of unique values for each column in the dataset\n",
        "df.nunique()"
      ],
      "metadata": {
        "id": "Ml_3SGBusB6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of unique values in the InvoiceNo column\n",
        "transactions = df['InvoiceNo'].nunique()\n",
        "\n",
        "# Count the number of unique values in the StockCode column\n",
        "products_bought = df['StockCode'].nunique()\n",
        "\n",
        "# Count the number of unique values in the CustomerID column\n",
        "customers =df['CustomerID'].nunique()\n",
        "\n",
        "# Calculate the percentage of missing customer information\n",
        "missing_customers = round(df['CustomerID'].isnull().sum() * 100 / len(df), 2)\n",
        "\n",
        "# Count the number of unique values in the Country column\n",
        "countries = df['Country'].nunique()\n",
        "\n",
        "# Print the results\n",
        "print(\"Number of transactions: \", transactions)\n",
        "print(\"Number of products bought: \", products_bought)\n",
        "print(\"Number of customers:\", customers)\n",
        "print(\"Percentage of customers NA: \", missing_customers, \"%\")\n",
        "print('Number of countries: ', countries)\n"
      ],
      "metadata": {
        "id": "Ifr_IucesJIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the unique values of the StockCode column\n",
        "unique_stock_codes = df.StockCode.unique()\n",
        "\n",
        "# Get the shape (number of rows and columns) of the unique values of the StockCode column\n",
        "stock_code_shape = unique_stock_codes.shape\n",
        "\n",
        "# Print the result\n",
        "print(\"Shape of unique values of StockCode column:\", stock_code_shape)"
      ],
      "metadata": {
        "id": "g_y1Ps8htXg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the unique values in the Description column\n",
        "unique_descriptions = df.Description.unique()\n",
        "\n",
        "# Get the number of unique values in the Description column\n",
        "num_unique_descriptions = unique_descriptions.shape\n",
        "\n",
        "# Print the result\n",
        "print(\"Number of unique descriptions:\", num_unique_descriptions[0])\n",
        "     "
      ],
      "metadata": {
        "id": "3XFXkuZSuTOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The descriptions of the items in the dataset show that there are more descriptions than stock codes, meaning that some stock codes have multiple descriptions associated with them."
      ],
      "metadata": {
        "id": "-swoTMyeunBw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Group the retail data by the StockCode and Description columns\n",
        "cat_des_df = df.groupby([\"StockCode\", \"Description\"]).count().reset_index()"
      ],
      "metadata": {
        "id": "UmQJZRwzvR1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the StockCode value counts where the count is greater than 1\n",
        "# Reset the index and get the first 10 rows\n",
        "head_stock_codes = (cat_des_df.StockCode.value_counts()[cat_des_df.StockCode.value_counts() > 1]\n",
        "                    .reset_index().head(10))\n",
        "\n",
        "# Show the result\n",
        "head_stock_codes\n",
        "     "
      ],
      "metadata": {
        "id": "MpL7Gp8OvY6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Get the 7th most common StockCode\n",
        "selected_stock_code = cat_des_df.StockCode.value_counts().reset_index()['index'][6]\n",
        "\n",
        "# Get the unique descriptions for the selected stock code\n",
        "unique_descriptions = df[df['StockCode'] == selected_stock_code]['Description'].unique()\n",
        "\n",
        "# Print the result\n",
        "print(\"Unique descriptions for stock code\", selected_stock_code, \":\", unique_descriptions)"
      ],
      "metadata": {
        "id": "_JkaohAuv3gy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready."
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "LZA2RW7RCxnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mis_val(d):\n",
        "    l=[]\n",
        "    for x in d.isnull().sum().index:\n",
        "        g=[]\n",
        "        g.append(x)\n",
        "        g.append(d[x].isnull().sum())\n",
        "        per=(d[x].isnull().sum()/len(d))*100\n",
        "        g.append(per)\n",
        "        l.append(g)\n",
        "    misd=pd.DataFrame(l)\n",
        "    misd.columns=['Column','Missing Value','Percentage of Missing Value']\n",
        "    misd.sort_values(by='Percentage of Missing Value',ascending=False,inplace=True)\n",
        "    return misd"
      ],
      "metadata": {
        "id": "UzL1g3e1C1pH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mis_val(df)"
      ],
      "metadata": {
        "id": "UgISHyOMC5QF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " handling missing values "
      ],
      "metadata": {
        "id": "D5HzP7mgJ3yY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "fTuiC6eJ-EJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows with missing values\n",
        "df.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "cb4SGSa_4GLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "handling outliers"
      ],
      "metadata": {
        "id": "ByT6_i01J-2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Calculate z-scores for numerical columns excluding datetime columns\n",
        "numerical_columns = df.select_dtypes(include=np.number).columns\n",
        "z_scores = (df[numerical_columns] - df[numerical_columns].mean()) / df[numerical_columns].std()\n",
        "\n",
        "# Define a threshold for outlier detection (e.g., z-score > 3)\n",
        "outlier_threshold = 3\n",
        "\n",
        "# Identify outliers based on z-scores and remove them\n",
        "df_no_outliers = df[(z_scores < outlier_threshold).all(axis=1)]\n"
      ],
      "metadata": {
        "id": "PMaiLDiUJv7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_no_outliers"
      ],
      "metadata": {
        "id": "LxBq1vEo7XZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "handling inconsistency "
      ],
      "metadata": {
        "id": "MIXQbcunKK0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate over each column\n",
        "for column in df_no_outliers.columns:\n",
        "    # Check if the column has inconsistent values\n",
        "    if len(df_no_outliers[column].unique()) > 1:\n",
        "        print(f\"Inconsistencies found in column: {column}\")\n"
      ],
      "metadata": {
        "id": "ykNNsMYsXAkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming your dataset is in a pandas DataFrame called 'df'\n",
        "\n",
        "# Find the count of inconsistencies in each column\n",
        "inconsistency_counts = (df_no_outliers != df_no_outliers.shift()).sum()\n",
        "\n",
        "# Get the columns with inconsistencies\n",
        "columns_with_inconsistencies = inconsistency_counts[inconsistency_counts > 0].index.tolist()\n",
        "\n",
        "# Print the inconsistency counts for each column\n",
        "for column in columns_with_inconsistencies:\n",
        "    inconsistency_count = inconsistency_counts[column]\n",
        "    print(f\"Inconsistency count in column '{column}': {inconsistency_count}\")\n",
        "\n",
        "# Remove inconsistencies from all columns\n",
        "df_cleaned = df_no_outliers.dropna().ffill()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0vdJOZtva2_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cleaned"
      ],
      "metadata": {
        "id": "qHNOWxkebGck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# dropping duplicates"
      ],
      "metadata": {
        "id": "OjY8wBZoKGhf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape before dropping duplicates\", df_cleaned.shape)\n",
        "df_cleaned = df_cleaned.drop_duplicates()\n",
        "print(\"Shape after dropping duplicates\", df_cleaned.shape)"
      ],
      "metadata": {
        "id": "Gq1dkrR3KERR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cleaned.shape"
      ],
      "metadata": {
        "id": "7w-bmWSLd7T2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "Mvcan2KCjQ_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " we handle missing values by filling them with the mean value of each column using the fillna() function."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming your dataset is in a pandas DataFrame called 'df' with a 'Country' column\n",
        "\n",
        "# Group the data by 'Country' and count the number of orders\n",
        "country_orders =df_cleaned['Country'].value_counts()\n",
        "\n",
        "# Get the country with the maximum number of orders\n",
        "country_with_max_orders = country_orders.idxmax()\n",
        "\n",
        "# Print the country with the maximum number of orders\n",
        "print(\"Country with maximum orders:\", country_with_max_orders)\n"
      ],
      "metadata": {
        "id": "iUYfHdeqMsRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming your dataset is in a pandas DataFrame called 'df_cleaned' with a 'Country' column\n",
        "\n",
        "# Group the data by 'Country' and count the number of orders\n",
        "country_orders = df_cleaned['Country'].value_counts().head(10)\n",
        "\n",
        "# Plot the bar chart\n",
        "plt.figure(figsize=(10, 6))\n",
        "country_orders.plot(kind='bar')\n",
        "plt.xlabel('Country')\n",
        "plt.ylabel('Number of Orders')\n",
        "\n",
        "# Add the number of orders as labels on top of the bars\n",
        "for i, v in enumerate(country_orders):\n",
        "    plt.text(i, v + 10, str(v), ha='center', va='bottom', fontsize=8)\n",
        "\n",
        "plt.title('Top 10 Countries with Maximum Orders')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "tcvd7iZmPJDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have chosen this graph to identify the country with the highest number of orders."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "After visualizing the graph, we can observe that the United Kingdom has the highest number of orders, followed by Germany and France."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The insights gained from identifying countries with the highest number of orders have the potential to create a positive business impact by allowing businesses to target those markets and allocate resources effectively. This can lead to increased sales, customer satisfaction, and overall growth. However, there are certain considerations that could result in negative growth. These include market saturation, geographic constraints, cultural factors, and economic conditions. Market saturation and intense competition in identified countries may limit growth opportunities. Geographic constraints such as high shipping costs or infrastructure limitations can hinder efficient operations. Cultural factors and failure to adapt to local preferences may negatively impact customer engagement. Additionally, economic instability or limited purchasing power in certain countries can present challenges to sustainable growth. To mitigate negative impacts, businesses should conduct thorough research, tailor strategies, and adapt to specific market conditions to maximize the positive impact of the gained insights."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_cleaned.columns "
      ],
      "metadata": {
        "id": "a-int_77j-Em"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will look for invoices with the letter \"c\" in the InvoiceNo column to see if there is an invoice with the quantity of -80995. If so, this would confirm that negative quantities correspond to cancelled orders."
      ],
      "metadata": {
        "id": "K9BaCXU0wqP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a variable 'cancelled_orders' to store all the rows in 'df_cleaned' where the value in the 'InvoiceNo' column contains the character 'C'\n",
        "cancelled_orders = df_cleaned[df_cleaned['InvoiceNo'].astype(str).str.contains('C')]\n",
        "\n",
        "# Display the first 5 rows of 'cancelled_orders' dataframe\n",
        "cancelled_orders.head()\n",
        "     "
      ],
      "metadata": {
        "id": "RiRmtMAIwpkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find a transaction where the quantity is -80995\n",
        "cancelled_orders[cancelled_orders['Quantity']==-80995]"
      ],
      "metadata": {
        "id": "2H2wwkqE6-Kg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the number of unique invoice numbers in the df_cleaned DataFrame\n",
        "total_orders_count = df_cleaned['InvoiceNo'].nunique()\n",
        "\n",
        "# Get the number of rows in the cancelled_orders DataFrame\n",
        "cancelled_orders_count = len(cancelled_orders)\n",
        "\n",
        "# Calculate the percentage of cancelled orders as a fraction of the total number of orders\n",
        "cancelled_orders_percentage = cancelled_orders_count / total_orders_count * 100\n",
        "\n",
        "# Print the results\n",
        "print(f\"Number of cancelled orders: {cancelled_orders_count}\")\n",
        "print(f\"Percentage of cancelled orders: {cancelled_orders_percentage:.2f}%\")"
      ],
      "metadata": {
        "id": "xsD_nCdm7Jna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine the number of cancelled orders for each country\n",
        "cancel_order_country = cancelled_orders.groupby('Country')['InvoiceNo'].count().reset_index()\n",
        "\n",
        "# Sort the country-wise count of cancelled orders in descending order and get the top 10 countries\n",
        "cancel_order_country_top10 = cancel_order_country.sort_values('InvoiceNo',ascending=False, ignore_index=True).head(10)\n",
        "cancel_order_country_top10"
      ],
      "metadata": {
        "id": "axhqxQtK7n9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Set the size of the figure to be displayed\n",
        "plt.figure(figsize=(14,8))\n",
        "\n",
        "# Plot the bar chart using Seaborn\n",
        "sns.barplot(x='Country', y='InvoiceNo', data=cancel_order_country_top10)\n",
        "\n",
        "# Add annotations on the bar charts\n",
        "for i, v in enumerate(cancel_order_country_top10['InvoiceNo']):\n",
        "    plt.text(i, v+50, str(v), ha='center', fontweight='bold')\n",
        "\n",
        "# Add labels and title to the chart\n",
        "plt.xlabel('Country') # Label for the x-axis\n",
        "plt.ylabel('Number of Cancelled Orders') # Label for the y-axis\n",
        "plt.title('Number of Cancelled Orders in Top 10 Countries') # Title for the chart\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jr855PB_CCaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have chosen this graph to determine the country with the highest number of cancelled orders."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.  A large number of orders, 35.86%, were cancelled\n",
        "\n",
        "2. The country with the highest number of cancelled orders is the United Kingdom, followed by Germany in the second position, and Eire in the third position."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "_8aJPm8WUTij"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights gained from identifying the countries with the highest number of cancelled orders, namely the United Kingdom, Germany, and Eire, can have both positive and negative implications for business growth. On the positive side, understanding these countries allows businesses to focus on improving customer satisfaction, refining product offerings, and optimizing logistics, which can lead to increased customer retention and positive business impact. However, the presence of high cancellations in specific countries can also indicate negative growth factors such as customer dissatisfaction, market viability challenges, or operational inefficiencies. It is crucial for businesses to analyze these insights and take appropriate actions to address any negative trends and drive positive growth."
      ],
      "metadata": {
        "id": "qPmqERLRUK8l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group the retail data by customer ID and count the number of unique invoices for each customer ID\n",
        "groupby_customers = pd.DataFrame(df_cleaned.groupby(\"CustomerID\")[\"InvoiceNo\"].nunique())\n",
        "\n",
        "# Display the first 5 rows of the resulting dataframe\n",
        "print(groupby_customers.head())"
      ],
      "metadata": {
        "id": "u7_IWlwnZNju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Display summary statistics for the groupby_customers dataframe\n",
        "groupby_customers.describe()"
      ],
      "metadata": {
        "id": "vAK21-ETZWlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On average, each customer places 5 orders."
      ],
      "metadata": {
        "id": "IUJZqUso9uPP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Group the retail_df dataframe by InvoiceNo and CustomerID\n",
        "temp_df = df_cleaned.groupby(['InvoiceNo','CustomerID'], as_index=False)\n",
        "\n",
        "# Count the number of products in each invoice\n",
        "temp_df = temp_df['InvoiceDate'].count()\n",
        "\n",
        "# Rename the column 'InvoiceDate' to 'Number of products'\n",
        "transaction_df = temp_df.rename(columns = {'InvoiceDate':'Number of products'})\n",
        "\n",
        "# Print the result\n",
        "print(transaction_df.head())"
      ],
      "metadata": {
        "id": "qdKIGjnw9-TQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the size of the plot\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Plot the distribution of the number of products in each transaction\n",
        "sns.distplot(transaction_df['Number of products'], color='purple')\n",
        "\n",
        "# Add a title to the plot\n",
        "plt.title(\"Distribution of the number of products in each transaction\", fontsize=16, fontweight='bold')\n",
        "\n",
        "# Add a label to the x-axis\n",
        "plt.xlabel(\"Number of products\", fontsize=14)\n",
        "\n",
        "# Add a label to the y-axis\n",
        "plt.ylabel(\"Density\", fontsize=14)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "P4S5ikSmAJX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A histogram was chosen to visualize the distribution of the number of products in each transaction because it allows us to see the frequency of different values in a continuous variable."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The distribution of the number of products in each transaction is heavily skewed. The majority of customers purchase fewer than 25 items in a single transaction.\n",
        "\n"
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights gained from this histogram could be used to inform decisions about inventory management and product pricing. For example, if the data shows that most transactions involve only one or two products, a business may decide to focus on promoting those particular products or adjusting pricing to encourage customers to purchase more items per transaction."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Description_df=df_cleaned['Description'].value_counts().reset_index()\n",
        "Description_df.rename(columns={'index': 'Description_Name'}, inplace=True)\n",
        "Description_df.rename(columns={'Description': 'Count'}, inplace=True)\n",
        "#top 5 Description Name\n",
        "Description_df.head()"
      ],
      "metadata": {
        "id": "miBgixGXBaPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot top 5 product name\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.title('Top 5 Product Name')\n",
        "sns.barplot(x='Count',y='Description_Name',data=Description_df[:5], palette='spring_r');"
      ],
      "metadata": {
        "id": "r05N23XWCDXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#bottom 5 description name\n",
        "Description_df.tail()"
      ],
      "metadata": {
        "id": "_hANaLVDCNam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot bottom 5 product name\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.title('Bottom 5 Product Name')\n",
        "sns.barplot(x='Count',y='Description_Name',data=Description_df[-5:], palette='spring_r');"
      ],
      "metadata": {
        "id": "LLk9Zf9NCVVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have selected this chart to know which products are top buyied products and  which are bottom products "
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Top product based on maximum selling are :**\n",
        "\n",
        "WHITE HANGING HEART T-LIGHT HOLDER,\tREGENCY CAKESTAND 3 TIER\t,\n",
        "JUMBO BAG RED RETROSPOT\t, PARTY BUNTING\t,LUNCH BAG RED RETROSPOT\t\n",
        "\n",
        "**Bottom 5 Product based on the selling are:**\n",
        "\n",
        "PC CUTLERY SET PANTRY DESIGN\t,LILY BROOCH WHITE/SILVER COLOUR\t, on cargo order\t,damages/dotcom?\t,PAPER CRAFT , LITTLE BIRDIE\t"
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These insights can help create a positive business impact by providing information on the top-selling products, which can be leveraged to drive sales, marketing, and inventory strategies. The company can focus on promoting and stocking the top-selling products to meet customer demand and maximize revenue.\n",
        "\n",
        "On the other hand, the insights on the bottom-selling products can help identify potential areas of improvement. The company can assess the reasons behind the low sales and evaluate whether it's due to product quality, lack of customer interest, or other factors. This information can guide decision-making processes such as product discontinuation, targeted marketing campaigns, or product enhancements to stimulate growth.\n",
        "\n",
        "Overall, the gained insights can contribute to making informed business decisions and driving positive growth, both by capitalizing on successful products and addressing challenges associated with underperforming products."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cleaned.describe().transpose()"
      ],
      "metadata": {
        "id": "3zcVWNAQHYaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have selected this graph to determine the average quantity of items ordered overall and the average unit price of each item."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The average quantity of items ordered overall is approximately 8, and the average unit price of each item is approximately 3."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The average quantity of items ordered overall being approximately 8 indicates that customers tend to purchase a reasonable quantity of items, which suggests a healthy level of demand. This insight can help businesses optimize their inventory management, pricing strategies, and promotional activities to meet customer expectations and maximize sales.\n",
        "\n",
        "Similarly, the average unit price of each item being approximately 3 provides valuable information about the pricing structure. Businesses can use this insight to assess their pricing competitiveness in the market, adjust prices if necessary, and ensure they are offering products at a competitive price point to attract customers."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "order_num = df_cleaned.groupby(['CustomerID'])[['InvoiceNo']].nunique().apply(display)"
      ],
      "metadata": {
        "id": "YQihJ_Y1JE4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, there are 4339 total customers. Let's see the number of repeat customers i.e. those who made >1 order"
      ],
      "metadata": {
        "id": "Ns6MNfhKJc6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mul_orders = pd.DataFrame(df_cleaned.groupby(['CustomerID'])[['InvoiceNo']].nunique())\n",
        "mul_orders['InvoiceNo']=mul_orders['InvoiceNo'].astype(int)\n",
        "mul_orders=mul_orders[mul_orders['InvoiceNo']>1]\n",
        "len(mul_orders)"
      ],
      "metadata": {
        "id": "0iO2UPeEJTQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, 3043 customers made a repeat purchase. That means, around 70% customers were retained.\n",
        "\n"
      ],
      "metadata": {
        "id": "TBrgOW69Jk3I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " \n",
        "I selected this specific chart to determine the number of repeat customers.\n",
        "\n"
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upon analyzing the data, we can observe that out of the total 4339 customers, approximately 3043 of them made a repeat purchase. This indicates a customer retention rate of around 70%, suggesting a significant portion of customers were retained and made multiple purchases."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The fact that around 70% of customers made a repeat purchase indicates a high level of customer loyalty and satisfaction. This insight suggests that the business has been successful in retaining customers and building long-term relationships.  \n",
        "\n",
        "The insights on repeat customers can contribute to a positive business impact by highlighting the success of customer retention efforts. However, it is important to continuously monitor and address any potential negative growth factors to ensure sustained success and mitigate any risks that may impact the business's growth trajectory."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cleaned['InvoiceDate']=pd.to_datetime(df_cleaned['InvoiceDate'])\n",
        "def get_month(x):\n",
        "    return dt.datetime(x.year, x.month, 1)\n",
        "  \n",
        "# Create the invoicemonth period column\n",
        "df_cleaned['InvoiceMonthYear'] = df_cleaned['InvoiceDate'].apply(get_month)\n",
        "df_cleaned.head()"
      ],
      "metadata": {
        "id": "ErViFF8MG4fg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grouping = df_cleaned.groupby('CustomerID')['InvoiceMonthYear']\n",
        "#finding and assigning earliest date of joining for each customer, here cohort refers to the time of joining\n",
        "df_cleaned['CohortMonthYear'] = grouping.transform('min')\n",
        "df_cleaned"
      ],
      "metadata": {
        "id": "GJ8uaTW8G89M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_cleaned['InvoiceMonthYear']=pd.to_datetime(df_cleaned['InvoiceMonthYear'])\n",
        "df_cleaned['CohortMonthYear']=pd.to_datetime(df_cleaned['CohortMonthYear'])\n",
        "\n",
        "  \n",
        "invoice_year, invoice_month = df_cleaned['InvoiceMonthYear'].dt.year,df_cleaned['InvoiceMonthYear'].dt.month\n",
        "cohort_year, cohort_month = df_cleaned['CohortMonthYear'].dt.year,df_cleaned['CohortMonthYear'].dt.month\n",
        "years_diff = invoice_year - cohort_year\n",
        "months_diff = invoice_month - cohort_month\n",
        "df_cleaned['MonthsRetained'] = years_diff * 12 + months_diff + 1\n",
        "df_cleaned.head()\n"
      ],
      "metadata": {
        "id": "kHQbQ8DrPtkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "months_count = df_cleaned['MonthsRetained'].value_counts().sort_index()\n",
        "months_count"
      ],
      "metadata": {
        "id": "LDvL9g-4FQZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create pie chart\n",
        "plt.pie(months_count, labels=months_count.index, autopct='%1.1f%%')\n",
        "plt.title(\"months customer retained\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3kThwjrUJelw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the line graph\n",
        "plt.plot(months_count.index, months_count.values, marker='o', linestyle='-')\n",
        "plt.xlabel('Retention Months')\n",
        "plt.ylabel('Number of Customers')\n",
        "plt.title('Customer Retention Over Months')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JLy53bGsNE3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have chosen this specific chart because it provides insights into the duration for which each customer has remained engaged with the company. While we have observed that customers are retaining, it is crucial to delve deeper and understand the specific length of time that customers have maintained their association with the company."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A significant portion of customers, approximately 28%, stayed with the company for only one month. However, as we progress to later months, the number of customers gradually decreases. This trend indicates a decline in customer retention over time. "
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gained insights on customer retention and the declining customer numbers provide an opportunity for businesses to take action and create a positive impact. By focusing on improving customer retention, addressing pain points, and implementing effective strategies to enhance engagement and loyalty, businesses can mitigate negative growth risks and foster sustainable growth."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "group = df_cleaned.groupby(['CohortMonthYear', 'MonthsRetained'])\n",
        "cohorts = group['CustomerID'].apply(pd.Series.nunique).reset_index()\n",
        "cohort_counts = cohorts.pivot(index='CohortMonthYear', columns='MonthsRetained', values='CustomerID')\n",
        "cohort_sizes = cohort_counts.iloc[:,0]\n",
        "retention_percent = cohort_counts.divide(cohort_sizes, axis=0)*10"
      ],
      "metadata": {
        "id": "AHXkd7HuPN8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "month_list = retention_percent.reset_index()['CohortMonthYear']\n",
        "\n",
        "def get_month_name(x):\n",
        "   return dt.datetime.strftime(x, '%b-%y')\n",
        "  \n",
        "month_list = month_list.apply(get_month_name)"
      ],
      "metadata": {
        "id": "ZpzghmR9QeK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,7))\n",
        "plt.title('Percentage Retention by Cohort')\n",
        "sns.heatmap(data=retention_percent,annot = True,vmin = 0.0, cmap=\"BuPu\",vmax = list(retention_percent.max().sort_values(ascending = False))[1]+3,fmt = '.1f',yticklabels=month_list)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1I4pQEpAQiOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " to check retention rate of customers cohortwise."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the cohort analysis graph of Percentage Retention of cohorts, we deduce following observations:\n",
        "\n",
        "1. Customers who purchased first time in the month of december 2010, there was significant decline in customers purchasing in the next 11 months but at the end of the year the customers came back to purchase the products in the store by a huge margin ."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the gained insights can have a positive business impact by leveraging the observed patterns in customer behavior to inform targeted marketing strategies. By capitalizing on the holiday season and understanding the factors contributing to customer decline, businesses can enhance customer retention, drive sales, and mitigate negative growth risks."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have a DataFrame called 'df_cleaned' with a 'UnitPrice' column\n",
        "\n",
        "# Calculate the average unit price\n",
        "average_unit_price = df_cleaned['UnitPrice'].mean()\n",
        "\n",
        "# Create the discount column based on the unit price\n",
        "df_cleaned['Discount'] = df_cleaned['UnitPrice'] < average_unit_price\n",
        "\n",
        "# Print the updated DataFrame\n",
        "df_cleaned\n"
      ],
      "metadata": {
        "id": "yWo0NBzARyzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Assuming you have a DataFrame called 'df_cleaned with a 'Month' column representing the months and a 'Discount' column indicating whether a discount was given or not\n",
        "\n",
        "# Group the data by month and check the presence of discounts\n",
        "discounts_by_month = df_cleaned.groupby('MonthsRetained')['Discount'].sum()\n",
        "\n",
        "# Create a new DataFrame to hold the results\n",
        "discounts_df = pd.DataFrame({'MonthsRetained': discounts_by_month.index, 'Discounts Given': discounts_by_month.values})\n",
        "\n",
        "# Print the discounts by month\n",
        "print(discounts_df)\n"
      ],
      "metadata": {
        "id": "lg7DsAOYSF5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Assuming you have a DataFrame called 'discounts_df' with 'MonthsRetained' and 'Discounts Given' columns\n",
        "\n",
        "# Plot the graph\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(discounts_df['MonthsRetained'], discounts_df['Discounts Given'], marker='o')\n",
        "plt.xlabel('Months ')\n",
        "plt.ylabel('Discounts Given')\n",
        "plt.title('Discounts Given by Months')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "2x8aS8rRTo4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "As observed, the retention rate is higher in the first month, which could be attributed to sales and discounts. To further analyze the impact of discounts, I have chosen to plot a graph showcasing the distribution of discounts given across different months."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The graph displays the number of discounts given in each month, providing insights that in 1st month  discounts were more prevalent."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This information can help identify patterns and trends in discount offerings, allowing businesses to optimize their marketing and sales strategies accordingly. By understanding the months with higher discount activity, businesses can plan targeted promotional campaigns and leverage discounts as a means to attract and retain customers, potentially leading to increased sales and customer loyalty."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the 'InvoiceDate' column to datetime type\n",
        "df_cleaned['InvoiceDate'] = pd.to_datetime(df_cleaned['InvoiceDate'])\n",
        "\n",
        "# Group the data by month and count the number of invoices\n",
        "invoices_by_month = df_cleaned.groupby(df_cleaned['InvoiceDate'].dt.to_period('M')).size()\n",
        "\n",
        "# Plot the graph\n",
        "plt.figure(figsize=(10, 6))\n",
        "invoices_by_month.plot(marker='o')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Number of Invoices')\n",
        "plt.title('Trend in Number of Invoices Over Time')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "06mzxR0waRW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To analyze the overall trend in the number of invoices over time and identify any seasonal patterns or significant changes in invoice frequency"
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The analysis of the number of invoices over time reveals fluctuations throughout the year, indicating both ups and downs in the invoice frequency. Additionally, a clear seasonal pattern emerges, with the highest number of invoices observed in November. This suggests a potential peak in sales or business activity during that month."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gained insights regarding the fluctuations in the number of invoices over time and the seasonal pattern can potentially help create a positive business impact. By understanding the ups and downs in invoice frequency and identifying the peak sales period in November, businesses can strategically plan their operations, marketing campaigns, and inventory management to optimize their sales during the high-demand season. This can lead to increased revenue and profitability.\n",
        "\n",
        "However, there might be insights that could lead to negative growth if not properly addressed"
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group the data by month and calculate the percentage of invoices with discounts\n",
        "discounts_by_month = df_cleaned.groupby('InvoiceMonthYear')['Discount'].mean() * 100\n",
        "\n",
        "# Plot the discounts by month\n",
        "plt.figure(figsize=(10, 6))\n",
        "discounts_by_month.plot(kind='bar')\n",
        "\n",
        "# Add percentage values to the plot\n",
        "for i, v in enumerate(discounts_by_month):\n",
        "    plt.text(i, v, f'{v:.1f}%', ha='center', va='bottom')\n",
        "\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Percentage of Invoices with Discounts')\n",
        "plt.title('Prevalence of Discounts by Month')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "abNaTCgNjr0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " to check are there any specific months or periods where discounts are more prevalent?  and How do discounts affect customer behavior and sales volume? "
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The summary statistics for discounted invoices show that:\n",
        "\n",
        "1. The average number of unique invoices for customers who received discounts is approximately 5.41.\n",
        "2. The average total invoice amount for discounted invoices is around 1246.32.\n",
        "3. The minimum total invoice amount is negative (-168466.70), indicating potential refunds or credits given for the discounted invoices.\n",
        "\n",
        "\n",
        "Similarly, for non-discounted invoices, the summary statistics show that:\n",
        "\n",
        "1. The average number of unique invoices for customers who did not receive discounts is approximately 4.68.\n",
        "2. The average total invoice amount for non-discounted invoices is around 980.44.\n",
        "The minimum total invoice amount is negative (-4522.50), indicating potential refunds or credits given for non-discounted invoices.\n"
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights gained from analyzing discounted and non-discounted invoices can have a positive impact on business: Understanding customer behavior and Tailoring strategies.\n",
        "\n",
        "However, there are potential negative impacts to consider: Variability in invoice amounts,\n",
        "Negative minimum invoice amounts,\n",
        "To mitigate negative impacts, businesses should address customer concerns, improve product offerings, and evaluate the effectiveness and profitability of discount programs regularly.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate purchase frequency by cohort month-year\n",
        "purchase_frequency = df_cleaned.groupby(['CohortMonthYear', 'CustomerID'])['InvoiceNo'].nunique().reset_index()\n",
        "\n",
        "# Calculate average purchase frequency by cohort month-year\n",
        "average_purchase_frequency = purchase_frequency.groupby('CohortMonthYear')['InvoiceNo'].mean()\n",
        "\n",
        "# Sort the average purchase frequency in descending order\n",
        "average_purchase_frequency_sorted = average_purchase_frequency.sort_values(ascending=False)\n",
        "\n",
        "df_cleaned['CohortMonthYear'] = df_cleaned['InvoiceDate'].dt.to_period('M')\n",
        "\n"
      ],
      "metadata": {
        "id": "aW6DP0dtzscw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "average_purchase_frequency_sorted.plot(kind='bar')\n",
        "\n",
        "# Annotate each bar with its corresponding value\n",
        "for i, v in enumerate(average_purchase_frequency_sorted):\n",
        "    plt.text(i, v, f\"{v:.2f}\", ha='center', va='bottom')\n",
        "\n",
        "plt.xlabel('Cohort Month-Year')\n",
        "plt.ylabel('Average Purchase Frequency')\n",
        "plt.title('Average Purchase Frequency by Cohort Month-Year')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GrcImvhEzBnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " to check What is the average purchase frequency for customers based on their cohort month-yearand  Are there any specific cohorts that show higher or lower purchase frequency."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The graph displays the average purchase frequency for customers based on their cohort month-year, with the corresponding values annotated on each bar. Notably, the cohort '2010-12' demonstrates a higher purchase frequency, while the cohort '2011-12' exhibits a comparatively lower purchase frequency."
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " the insights gained from analyzing the purchase frequency by cohort month-year can inform businesses' decision-making processes and guide them towards actions that have the potential to generate positive business impact while addressing any negative growth areas."
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the total revenue by multiplying quantity and unit price\n",
        "df_cleaned['TotalRevenue'] = df_cleaned['Quantity'] * df_cleaned['UnitPrice']\n",
        "\n",
        "# Group the data by stock code and description and calculate the average unit price and total revenue\n",
        "stock_price_revenue = df_cleaned.groupby(['StockCode', 'Description']).agg({'UnitPrice': 'mean', 'TotalRevenue': 'sum'})\n",
        "\n",
        "# Sort the data by average unit price in descending order\n",
        "stock_price_revenue_sorted = stock_price_revenue.sort_values(by='UnitPrice', ascending=False)\n",
        "\n",
        "# Print the top 5 stock codes and descriptions with highest unit prices\n",
        "top_high_price_items = stock_price_revenue_sorted.head(5)\n",
        "print(\"Top 5 Stock Codes and Descriptions with Highest Unit Prices:\")\n",
        "print(top_high_price_items)\n",
        "\n",
        "# Calculate the contribution of high-priced items to overall revenue\n",
        "high_price_items_contribution = top_high_price_items['TotalRevenue'].sum() / df_cleaned['TotalRevenue'].sum() * 100\n",
        "print(\"\\nContribution of High-Priced Items to Overall Revenue: {:.2f}%\".format(high_price_items_contribution))\n"
      ],
      "metadata": {
        "id": "MS3lyUwEQGMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " to check are there any specific stock codes or descriptions that are associated with higher unit prices and  Do these high-priced items contribute significantly to the overall revenue?"
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 1. specific  descriptions that are associated with higher unit prices are \n",
        "  \n",
        "REGENCY MIRROR WITH SHUTTERS        \n",
        "DOTCOM POSTAGE                          \n",
        "RUSTIC  SEVENTEEN DRAWER SIDEBOARD       \n",
        "AMAZON FEE                           \n",
        "VINTAGE RED KITCHEN CABINET  \n",
        "\n",
        "2. Contribution of High-Priced Items to Overall Revenue: 1.05%"
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The specific descriptions associated with higher unit prices are \"REGENCY MIRROR WITH SHUTTERS,\" \"DOTCOM POSTAGE,\" \"RUSTIC SEVENTEEN DRAWER SIDEBOARD,\" \"AMAZON FEE,\" and \"VINTAGE RED KITCHEN CABINET.\" However, these high-priced items contribute only 1.05% to the overall revenue. While they may attract niche customers and enhance brand perception, their limited sales volume could lead to a narrower customer base and less revenue diversification. A comprehensive analysis is necessary to determine their specific impact on business growth."
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corr = df_cleaned.corr()\n",
        "plt.subplots(figsize=(15,10))\n",
        "sns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, annot=True, cmap=sns.diverging_palette(220, 20, as_cmap=True))\n",
        "sns.heatmap(corr, xticklabels=corr.columns,\n",
        "            yticklabels=corr.columns, \n",
        "            annot=True,\n",
        "            cmap=sns.diverging_palette(220, 20, as_cmap=True))"
      ],
      "metadata": {
        "id": "d3ZkvDVQ56O8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have picked up this chart because correlation heatmap visualize the strength of relationships between numerical variables."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that following have strong correlation :\n",
        "1. InvoiceTotal and Quantity\n",
        "2. InvoiceTotal and UnitPrice \n",
        "3. Discount and Quantity \n",
        "4. Discount and CustomerID\n",
        "5. MonthsRetained and CustomerID\n"
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot "
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns = ['Quantity', 'UnitPrice']\n",
        "\n",
        "# Create the pairplot\n",
        "sns.pairplot(df_cleaned[columns])\n",
        "plt.show()\n",
        "\n",
        "# Free up memory\n",
        "del data\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "u6xRfjfb3y9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pair plot is used to understand the best set of features to explain a relationship between two variables or to form the most separated clusters."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perform cohort analysis (a cohort is a group of subjects that share a defining characteristic). Observe how a cohort behaves across time and compare it to other cohorts."
      ],
      "metadata": {
        "id": "OhdpKIozu4uf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A cohort is a group of subjects who share a defining characteristic. We can observe how a cohort behaves across time and compare it to other cohorts."
      ],
      "metadata": {
        "id": "zdIwlMgAvGCs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Types of cohorts:\n",
        "* Time Cohorts:\n",
        "They are customers who signed up for a product or service during a particular time frame. Analyzing these cohorts shows the customers’ behavior depending on the time they started using the company’s products or services. The time may be monthly or quarterly even daily.\n",
        "\n",
        "* Behaviour Cohorts:\n",
        "They are customers who purchased a product or subscribed to a service in the past. It groups customers by the type of product or service they signed up. Customers who signed up for basic level services might have different needs than those who signed up for advanced services. Understaning the needs of the various cohorts can help a company design custom-made services or products for particular segments.\n",
        "\n",
        "* Size Cohorts:\n",
        "Size cohorts refer to the various sizes of customers who purchase company’s products or services. This categorization can be based on the amount of spending in some periodic time after acquisition or the product type that the customer spent most of their order amount in some period of time.\n",
        "\n",
        "For cohort analysis, there are a few labels that we have to create:\n",
        "\n",
        "Invoice period - A string representation of the year and month of a single transaction/invoice. Cohort group - A string representation of the the year and month of a customer’s first purchase. This label is common across all invoices for a particular customer. Cohort period/Index-  A integer representation a customer’s stage in its “lifetime”. The number represents the number of months passed since the first purchase."
      ],
      "metadata": {
        "id": "9Fb2CXx5vKGn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cohort =df_cleaned.copy()\n",
        "cohort"
      ],
      "metadata": {
        "id": "XxKR3ybGvUMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function that will parse the date\n",
        "import datetime\n",
        "def get_month(x):\n",
        "    return datetime.datetime(x.year,x.month,1) \n",
        "\n",
        "# Create InvoiceMonth column\n",
        "cohort['InvoiceMonth'] = cohort['InvoiceDate'].apply(get_month) \n",
        "\n",
        "# Group by CustomerID and select the InvoiceMonth value\n",
        "grouping = cohort.groupby('CustomerID')['InvoiceMonth']\n",
        "\n",
        "# Assign a minimum InvoiceMonth value to the dataset\n",
        "cohort['CohortMonth'] = grouping.transform('min')"
      ],
      "metadata": {
        "id": "5WCG9xgivb8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate time offset in months\n",
        "Calculating time offset for each transaction allows you to report the metrics for each cohort in a comparable fashion.\n",
        "\n",
        "First, we will create some variables that capture the integer value of years and months for Invoice and Cohort Date using the get_date_int() function"
      ],
      "metadata": {
        "id": "ahHJPaQBvgvn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_date_int(df, column):\n",
        "    year = df[column].dt.year\n",
        "    month = df[column].dt.month\n",
        "    return year, month\n",
        "\n",
        "# Get the integers for date parts from the `InvoiceMonth` column\n",
        "invoice_year, invoice_month = get_date_int(cohort,'InvoiceMonth')\n",
        "\n",
        "# Get the integers for date parts from the `CohortMonth` column\n",
        "cohort_year, cohort_month = get_date_int(cohort,'CohortMonth')"
      ],
      "metadata": {
        "id": "zpTSbAIAvjjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (\"Unique terms for Cohort Year is {} \" .format(cohort_year.unique()))\n",
        "print (\"Unique terms for Cohort Month is {} \" .format(cohort_month.unique()))\n",
        "print (\"Unique terms for Invoice Year is {} \" .format(invoice_year.unique()))\n",
        "print (\"Unique terms for Invoice Year is {} \" .format(invoice_month.unique()))"
      ],
      "metadata": {
        "id": "wnDg0uvBvnKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate difference in years\n",
        "years_diff = invoice_year - cohort_year\n",
        "\n",
        "# Calculate difference in months\n",
        "months_diff = invoice_month - cohort_month\n",
        "\n",
        "# Extract the difference in months from all previous values\n",
        "cohort['CohortIndex'] = years_diff * 12 + months_diff + 1"
      ],
      "metadata": {
        "id": "vav1DypuvrWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#THis Cohort Index will give us an idea on the time difference in months between the customer's first purchase and the customer's current purchase\n",
        "cohort['CohortIndex'].unique()\n",
        "cohort"
      ],
      "metadata": {
        "id": "X5hzJNy1vvJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate retention rate Customer retention is a very useful metric to understand how many of all the customers are still active. It gives you the percentage of active customers compared to the total number of customers"
      ],
      "metadata": {
        "id": "XfRopYhEvzYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grouping = cohort.groupby(['CohortMonth', 'CohortIndex'])"
      ],
      "metadata": {
        "id": "KZvLXFHqv2Gh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of unique values per customer ID\n",
        "#cohort_data = grouping['CustomerID'].apply(pd.Series.nunique).reset_index()\n",
        "cohort_data = grouping['CustomerID'].apply(pd.Series.nunique).reset_index()\n",
        "\n",
        "# Create a pivot \n",
        "cohort_counts = cohort_data.pivot(index='CohortMonth', columns='CohortIndex', values='CustomerID')\n",
        "\n",
        "# Select the first column and store it to cohort_sizes\n",
        "cohort_sizes = cohort_counts.iloc[:,0]\n",
        "\n",
        "# Divide the cohort count by cohort sizes along the rows\n",
        "retention = cohort_counts.divide(cohort_sizes, axis=0)*100\n",
        "#print (cohort[cohort['CohortMonth']=='2011-12-01']['CustomerID'].nunique()) #Verifies 41 against this month\n",
        "#cohort_sizes\n",
        "retention.index = retention.index.date"
      ],
      "metadata": {
        "id": "zTOYQIB8v5jr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "month_list = [\"Dec '10\", \"Jan '11\", \"Feb '11\", \"Mar '11\", \"Apr '11\",\\\n",
        "              \"May '11\", \"Jun '11\", \"Jul '11\", \"Aug '11\", \"Sep '11\", \\\n",
        "              \"Oct '11\", \"Nov '11\", \"Dec '11\"]\n",
        "\n",
        "# Initialize inches plot figure\n",
        "plt.figure(figsize=(15,13))\n",
        "\n",
        "# Add a title\n",
        "plt.title('Retention by Monthly Cohorts')\n",
        "\n",
        "# Create the heatmap\n",
        "ax = sns.heatmap(data=retention,\n",
        "            annot = True,\n",
        "            cmap = \"YlGnBu\",\n",
        "            vmin = 0.0,\n",
        "            vmax = list(retention.max().sort_values(ascending = False))[1]+3,\n",
        "            fmt = '.1f',\n",
        "            linewidth = 0.9,\n",
        "            yticklabels=month_list)\n",
        "\n",
        "# plot of the data\n",
        "bottom, top = ax.get_ylim()\n",
        "ax.set_ylim(bottom + 0.5, top - 0.5)\n",
        "fig = plt.figure()\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "lw8SJ1KRv9Dt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate average price per cohort Now we will calculate the average price metric and analyze if there are any differences in shopping patterns across time and across cohorts"
      ],
      "metadata": {
        "id": "TgAfxuuJwAPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a groupby object and pass the monthly cohort and cohort index as a list\n",
        "grouping = cohort.groupby(['CohortMonth', 'CohortIndex']) \n",
        "\n",
        "# Calculate the average of the unit price column\n",
        "cohort_data = grouping['UnitPrice'].mean()\n",
        "\n",
        "# Reset the index of cohort_data\n",
        "cohort_data = cohort_data.reset_index()\n",
        "\n",
        "# Create a pivot \n",
        "average_price = cohort_data.pivot(index='CohortMonth', columns='CohortIndex', values='UnitPrice')\n",
        "#average_price.round(1)\n",
        "#average_price.index = average_price.index.date\n",
        "average_price\n",
        "#cohort_data\n",
        "#cohort"
      ],
      "metadata": {
        "id": "3yzUPQ-OwBZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize plot figure\n",
        "plt.figure(figsize=(15, 13))\n",
        "\n",
        "plt.title('Average Spend per Monthly Cohorts')\n",
        "# Create the heatmap\n",
        "ax = sns.heatmap(data = average_price,\n",
        "            annot=True,\n",
        "            vmin = 0.0,\n",
        "#             vmax =20,\n",
        "            cmap='YlGnBu',\n",
        "            vmax = list(average_price.max().sort_values(ascending = False))[1]+3,\n",
        "            fmt = '.1f',\n",
        "            linewidth = 0.3,\n",
        "            yticklabels=month_list)\n",
        "bottom, top = ax.get_ylim()\n",
        "ax.set_ylim(bottom + 0.5, top - 0.5)\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "mC0D-HBJwGdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate average quantity per cohort\n",
        "Now we will calculate the average quantity metric and analyze if there are any differences in shopping patterns across time and across cohorts."
      ],
      "metadata": {
        "id": "UCLV9e4fwJYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a groupby object and pass the monthly cohort and cohort index as a list\n",
        "grouping = cohort.groupby(['CohortMonth', 'CohortIndex']) \n",
        "\n",
        "# Calculate the average of the Quantity column\n",
        "cohort_data = grouping['Quantity'].mean()\n",
        "\n",
        "# Reset the index of cohort_data\n",
        "cohort_data = cohort_data.reset_index()\n",
        "\n",
        "# Create a pivot \n",
        "average_quantity = cohort_data.pivot(index='CohortMonth', columns='CohortIndex', values='Quantity')\n",
        "average_quantity.round(1)\n",
        "average_quantity.index = average_quantity.index.date"
      ],
      "metadata": {
        "id": "n0QdZsSbwLsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize plot figure\n",
        "plt.figure(figsize=(15, 13))\n",
        "\n",
        "# Add a title\n",
        "plt.title('Average Quantity per Monthly Cohorts')\n",
        "\n",
        "# Create the heatmap\n",
        "ax  = sns.heatmap(data = average_quantity,\n",
        "            annot=True,\n",
        "            vmin = 0.0,\n",
        "            cmap='Pastel2',\n",
        "            vmax = list(average_quantity.max().sort_values(ascending = False))[1]+3,\n",
        "            fmt = '.1f',\n",
        "            linewidth = 0.3,\n",
        "            yticklabels=month_list)\n",
        "\n",
        "bottom, top = ax.get_ylim()\n",
        "ax.set_ylim(bottom + 0.5, top - 0.5)\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "66TdZjeOwOFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"Is there a significant difference in the average InvoiceTotal between customers who received a discount and customers who did not receive a discount?"
      ],
      "metadata": {
        "id": "dvrlMpZp_525"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Research Hypothesis:\n",
        "\n",
        "Null Hypothesis (H0): There is no significant difference in the average InvoiceTotal between customers who received a discount and customers who did not receive a discount.\n",
        "\n",
        "\n",
        "Alternative Hypothesis (HA): There is a significant difference in the average InvoiceTotal between customers who received a discount and customers who did not receive a discount."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the invoice total by multiplying quantity and unit price\n",
        "df_cleaned['InvoiceTotal'] = df_cleaned['Quantity'] * df_cleaned['UnitPrice']\n",
        "\n",
        "# Separate InvoiceTotal for discounted and non-discounted customers\n",
        "invoice_total_discounted = df_cleaned[df_cleaned['Discount'] > 0]['InvoiceTotal']\n",
        "invoice_total_non_discounted = df_cleaned[df_cleaned['Discount'] == 0]['InvoiceTotal']\n",
        "\n",
        "# Perform t-test\n",
        "t_statistic, p_value = stats.ttest_ind(invoice_total_discounted, invoice_total_non_discounted)\n",
        "\n",
        "# Print the t-statistic and p-value\n",
        "print(\"T-statistic:\", t_statistic)\n",
        "print(\"P-value:\", p_value)\n"
      ],
      "metadata": {
        "id": "0nhe1GGUAAB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "used The t-statistic"
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The t-statistic is a measure of the difference between the means of two groups, in this case, customers who received a discount and customers who did not receive a discount. The t-statistic value of -12.97 indicates a significant difference in the average InvoiceTotal between the two groups.\n",
        "\n",
        "The p-value is a measure of the strength of evidence against the null hypothesis. In this case, the p-value is very small (1.81), which is less than the commonly used significance level of 0.05. This indicates strong evidence to reject the null hypothesis.\n",
        "\n",
        "Therefore, based on the obtained results, we can conclude that there is a significant difference in the average InvoiceTotal between customers who received a discount and customers who did not receive a discount."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H0): There is no significant difference in the average revenue generated by customers in different cohorts.\n",
        "\n",
        "Alternate Hypothesis (HA): There is a significant difference in the average revenue generated by customers in different cohorts."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group the data by cohort month-year and calculate the total revenue\n",
        "cohort_revenue = df_cleaned.groupby('CohortMonthYear')['InvoiceTotal'].sum()\n",
        "\n",
        "# Perform one-way ANOVA to compare the average revenue between cohorts\n",
        "import scipy.stats as stats\n",
        "cohorts = df_cleaned['CohortMonthYear'].unique()\n",
        "groups = [df_cleaned[df_cleaned['CohortMonthYear'] == cohort]['InvoiceTotal'] for cohort in cohorts]\n",
        "fvalue, pvalue = stats.f_oneway(*groups)\n",
        "\n",
        "# Print the test results\n",
        "print(\"F-value:\", fvalue)\n",
        "print(\"P-value:\", pvalue)\n"
      ],
      "metadata": {
        "id": "0-H_AeBbC9tm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "used F-value."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The F-value represents the variation between the means of different cohorts.\n",
        "\n",
        "The obtained F-value of 3.34 and the corresponding p-value of 7.09e-05 indicate that there is a significant difference in the average revenue generated by customers in different cohorts. Therefore, we reject the null hypothesis and conclude that there is evidence to support the alternate hypothesis, suggesting that there is a significant difference in the average revenue between cohorts."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H0): There is no significant difference in the average InvoiceTotal between different quantities of items ordered.\n",
        "\n",
        "Alternate Hypothesis (HA): There is a significant difference in the average InvoiceTotal between different quantities of items ordered."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate InvoiceTotal for different quantities of items ordered\n",
        "invoice_total_quantity_low = df_cleaned[df_cleaned['Quantity'] < 10]['InvoiceTotal']\n",
        "invoice_total_quantity_high = df_cleaned[df_cleaned['Quantity'] >= 10]['InvoiceTotal']\n",
        "\n",
        "# Perform t-test\n",
        "t_statistic, p_value = stats.ttest_ind(invoice_total_quantity_low, invoice_total_quantity_high)\n",
        "\n",
        "# Print the t-statistic and p-value\n",
        "print(\"T-statistic:\", t_statistic)\n",
        "print(\"P-value:\", p_value)\n"
      ],
      "metadata": {
        "id": "wxJOVhqmYJjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "T-statistic test is used\n"
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " The t-statistic is chosen for hypothesis testing when comparing the means of two groups. \n",
        " \n",
        " A t-statistic of -40.64 indicates a significant difference in the average InvoiceTotal between different quantities of items ordered. The p-value of 0.0 further supports this finding. The p-value being very close to zero suggests strong evidence against the null hypothesis, indicating that the observed difference in average InvoiceTotal is unlikely due to chance. Therefore, we reject the null hypothesis and conclude that there is a significant difference in the average InvoiceTotal between different quantities of items ordered."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cleaned"
      ],
      "metadata": {
        "id": "ALBb_ROuVL4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cleaned.isnull().sum()"
      ],
      "metadata": {
        "id": "Kld3cblPU-Af"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cleaned.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "0JF1R9dEVWOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cleaned[df_cleaned.duplicated()].sum()"
      ],
      "metadata": {
        "id": "5ROuwVQxVkAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cleaned.drop_duplicates(inplace=True)"
      ],
      "metadata": {
        "id": "jOt4kWVNV66v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 8))  \n",
        "sns.boxplot(data=df_cleaned)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8ABRaghhWZ5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "here we dont need to handle outlier because we are dealing with textual columns."
      ],
      "metadata": {
        "id": "HBnsqm4RUQ-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kBppNHBGX9Y7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "no need"
      ],
      "metadata": {
        "id": "B3aXbIJKUdFI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing \n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cleaned.sample(2)"
      ],
      "metadata": {
        "id": "qAOmTVbyYmEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions\n",
        "import contractions"
      ],
      "metadata": {
        "id": "YXqXfEXhY5e8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cleaned['Description'] = df_cleaned['Description'].astype(str)\n",
        "df_cleaned['Description'] = df_cleaned['Description'].apply(lambda x: contractions.fix(x))\n",
        "\n"
      ],
      "metadata": {
        "id": "EuuvQssdY_Yi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cleaned['Country']= df_cleaned['Country'].apply(lambda x:contractions.fix(x)) "
      ],
      "metadata": {
        "id": "H30CCoKiUmte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "# Lower Casing\n",
        "df_cleaned['text_data'] = df_cleaned['Description'] + ' ' + df_cleaned['Country']\n",
        "df_cleaned['text_data'] = df_cleaned['text_data'].str.lower()\n"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "def remove_punctuation(text):\n",
        "    '''a function for removing punctuation'''\n",
        "    import string \n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    return text.translate(translator)\n",
        "    \n",
        "df_cleaned['text_data'] = df_cleaned['text_data'].apply(remove_punctuation)\n",
        "df_cleaned['text_data'].sample(5)"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits\n",
        "import re\n",
        "# Remove links\n",
        "\n",
        "df_cleaned['text_data'] = df_cleaned['text_data'].apply(lambda x: re.sub(r\"http\\S+\", \"\", x))\n",
        "\n",
        "# Remove digits\n",
        "df_cleaned['text_data'] = df_cleaned['text_data'].apply(lambda x: re.sub(r\"\\d+\", \"\", x))"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extracting the stopwords from nltk library\n",
        "stop_words = nltk.corpus.stopwords.words('english')\n",
        "print(stop_words)"
      ],
      "metadata": {
        "id": "m9iF6WqzZpxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stopwords(text):\n",
        "    '''a function for removing the stopword and lowercase the each word'''\n",
        "    text = [word.lower() for word in text.split() if word.lower() not in stop_words]\n",
        "    # joining the list of words with space separator\n",
        "    return \" \".join(text) "
      ],
      "metadata": {
        "id": "_OW64FbXZs9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# applying stopwords function.\n",
        "df_cleaned['text_data'] = df_cleaned['text_data'].apply(stopwords)"
      ],
      "metadata": {
        "id": "GYzd0SOAZvJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces\n",
        "df_cleaned['text_data'] = df_cleaned['text_data'].apply(lambda x: \" \".join(x.split()))"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cleaned['text_data'].sample(5)"
      ],
      "metadata": {
        "id": "svqhVOuuZ6A9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "df_cleaned['text_data'] = df_cleaned['text_data'].apply(nltk.word_tokenize)\n",
        "\n",
        "# Free up memory\n",
        "del data\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n",
        "import nltk\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')\n",
        "#applying Lemmatization\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Create a lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_tokens(tokens):\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    return lemmatized_tokens\n",
        "\n",
        "# Lemmatize the data columns column\n",
        "df_cleaned['text_data'] = df_cleaned['text_data'].apply(lemmatize_tokens)"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "df_cleaned['text_data'] = df_cleaned['text_data'].apply(nltk.pos_tag)\n",
        "df_cleaned['text_data'].head()\n",
        "\n",
        "# Free up memory\n",
        "del data\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "JDVbxBl_fH1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text\n",
        "bag_of_words = df_cleaned.text_data"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t_vectorizer = TfidfVectorizer(tokenizer=lambda x: x, lowercase=False,max_features=20000)\n",
        "X_tfidf= t_vectorizer.fit_transform(bag_of_words)   "
      ],
      "metadata": {
        "id": "jaMJhdQQa-3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_tfidf.shape  "
      ],
      "metadata": {
        "id": "jOgfLe8zbCXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert X into array form for clustering\n",
        "X = X_tfidf.toarray() \n",
        "X"
      ],
      "metadata": {
        "id": "EohZ2jPlbEEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "print(scaler.fit(X))\n",
        "\n",
        "# Free up memory\n",
        "del data\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)\n",
        "pca = PCA()\n",
        "pca.fit(X)  "
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(14,5), dpi=120)\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "plt.xlabel('number of components')\n",
        "plt.ylabel('cumulative explained variance')\n",
        "plt.title('The number of components needed to explain variance')\n",
        "plt.axhline(y=0.95, color='r', linestyle='dashdot')\n",
        "plt.text(0.5, 0.85, '95% cut-off threshold', color = 'green', fontsize=16)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Cu2-fJn6cbGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca_tuned = PCA(n_components=3)\n",
        "pca_tuned.fit(X)\n",
        "X_transformed = pca_tuned.transform(X)\n",
        "X_transformed.shape"
      ],
      "metadata": {
        "id": "TgoDnQ3Tcdkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # shape\n",
        "print(\"original shape: \", X.shape)\n",
        "print(\"transformed shape:\", X_transformed.shape)"
      ],
      "metadata": {
        "id": "MCT46J3kch3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely."
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why? "
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "# ML Model - 1 Implementation\n",
        "from sklearn.cluster import KMeans\n",
        "#Within Cluster Sum of Squared Errors(WCSS) for different values of k\n",
        "plt.figure(figsize=(10,6), dpi=120)\n",
        "wcss = []\n",
        "for i in range(1, 11):\n",
        "    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=20)\n",
        "    kmeans.fit(X_transformed)\n",
        "    wcss.append(kmeans.inertia_)"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#elbow curve\n",
        "plt.plot(range(1,11),wcss)\n",
        "plt.plot(range(1,11),wcss, linewidth=2, color=\"red\", marker =\"o\")\n",
        "plt.xlabel(\"K Value\", size = 20, color = 'purple')\n",
        "plt.xticks(np.arange(1,11,1))\n",
        "plt.ylabel(\"WCSS\", size = 20, color = 'green')\n",
        "plt.title('Elbow Curve', size = 20, color = 'blue')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TsLhZr3EdhX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#vizualizing the clusters and the datapoints in each clusters\n",
        "plt.figure(figsize = (10,6), dpi = 120)\n",
        "\n",
        "kmeans= KMeans(n_clusters = 5, init= 'k-means++', random_state = 42)\n",
        "kmeans.fit(X_transformed)\n",
        "\n",
        "#predict the labels of clusters.\n",
        "label = kmeans.fit_predict(X_transformed)\n",
        "#Getting unique labels\n",
        "unique_labels = np.unique(label)\n",
        " \n",
        "#plotting the results:\n",
        "for i in unique_labels:\n",
        "    plt.scatter(X_transformed[label == i , 0] , X_transformed[label == i , 1] , label = i)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qKjSo61Cdj1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "#sillhoute score of clusters \n",
        "sill = [] \n",
        "for i in range(2,21):\n",
        "    model = KMeans(n_clusters=i,init ='k-means++',random_state=51)\n",
        "    model.fit(X_transformed)\n",
        "    y1 = model.predict(X_transformed)\n",
        "    score = silhouette_score(X_transformed,y1)\n",
        "    sill.append(score)\n",
        "    print('cluster: %d \\t Sillhoute: %0.4f'%(i,score))"
      ],
      "metadata": {
        "id": "_3Wa6XqRdqgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score, silhouette_samples\n",
        "from yellowbrick.cluster import SilhouetteVisualizer\n",
        "     \n",
        "def silhouette_score_analysis(n):\n",
        "\n",
        "  for n_clusters in range(2,n):\n",
        "      km = KMeans (n_clusters=n_clusters, random_state=5)\n",
        "      preds = km.fit_predict(X_transformed)\n",
        "      centers = km.cluster_centers_\n",
        "\n",
        "      score = silhouette_score(X_transformed, preds, metric='euclidean')\n",
        "      print (\"For n_clusters = {}, silhouette score is {}\".format(n_clusters, score))\n",
        "\n",
        "      visualizer = SilhouetteVisualizer(km)\n",
        "\n",
        "      visualizer.fit(X_transformed) # Fit the training data to the visualizer\n",
        "      visualizer.show() # Draw/show/poof the data"
      ],
      "metadata": {
        "id": "PxUU0tC9dsyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "silhouette_score_analysis(21)"
      ],
      "metadata": {
        "id": "EXWL1JkQdvSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#importing module for hierarchial clustering and vizualizing dendograms\n",
        "import scipy.cluster.hierarchy as sch\n",
        "plt.figure(figsize=(20,15))\n",
        "dendrogram = sch.dendrogram(sch.linkage(X_transformed, method = 'ward'),orientation='top',show_leaf_counts=True)\n",
        "plt.axhline(y=6, color='r', linestyle='--')\n",
        "plt.title('Dendrogram')\n",
        "plt.xlabel('Content')\n",
        "plt.ylabel('Euclidean Distances')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3H639nLid4JV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# agglomerative clustering\n",
        "from numpy import unique\n",
        "from numpy import where\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "# define the model\n",
        "model = AgglomerativeClustering(n_clusters = 5)      #n_clusters=5\n",
        "# fit model and predict clusters\n",
        "y_hc = model.fit_predict(X_transformed)\n",
        "# retrieve unique clusters\n",
        "clusters = unique(y_hc)\n",
        "# create scatter plot for samples from each cluster\n",
        "for cluster in clusters:\n",
        "\t# get row indexes for samples with this cluster\n",
        "\trow_ix = where(y_hc == cluster)\n",
        "\t# create scatter of these samples\n",
        "\tplt.scatter(X_transformed[row_ix, 0], X_transformed[row_ix, 1])\n",
        "# show the plot\n",
        "plt.show()\n",
        "\n",
        "#Silhouette Coefficient\n",
        "print(\"Silhouette Coefficient: %0.3f\"%silhouette_score(X_transformed,y_hc, metric='euclidean'))\n",
        "\n",
        "#davies_bouldin_score of our clusters \n",
        "from sklearn.metrics import davies_bouldin_score\n",
        "davies_bouldin_score(X_transformed, y_hc)\n",
        "print(\"davies_bouldin_score %0.3f\"%davies_bouldin_score(X_transformed, y_hc))"
      ],
      "metadata": {
        "id": "wOrAdSDjd7KV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RFM Analysis\n",
        "RFM analysis is a customer segmentation technique that uses past purchase behavior to divide customers into groups. RFM helps divide customers into various categories or clusters to identify customers who are more likely to respond to promotions and also for future personalization services.\n",
        "\n",
        "Recency (R):\n",
        "Time since last purchase\n",
        "\n",
        "Frequency (F):\n",
        "Total number of purchases\n",
        "\n",
        "Monetary (M):\n",
        "Total purchase value"
      ],
      "metadata": {
        "id": "4F14LlrVwnuM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Benefits of RFM analysis\n",
        "Increased customer retention Increased response rate Increased conversion rate Increased revenue\n",
        "\n",
        "To perform RFM analysis, we divide customers into four equal groups according to the distribution of values for recency, frequency, and monetary value. Four equal groups across three variables create 64 (4x4x4) different customer segments, which is a manageable number.\n",
        "\n",
        "For example, let’s look at a customer who: is within the group who purchased most recently (R=4), is within the group who purchased most quantity (F=4), is within the group who spent the most (M=4) This customer belongs to RFM segment 4-4-4 (Best Customers), (R=4, F=4, M=4)"
      ],
      "metadata": {
        "id": "6P6GcL12wtUB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Segment =  ['Platinum Customers',\n",
        "                     'Big Spenders',\n",
        "                     'High Spend New Customers',\n",
        "                     'Lowest-Spending Active Loyal Customers',\n",
        "                     'Recent Customers',\n",
        "                     'Good Customers Almost Lost', \n",
        "                     'Churned Best Customers',\n",
        "                     'Lost Cheap Customers ']\n",
        "RFM = [ \n",
        "               ['444', '443'],\n",
        "               ['114', '124', '134', '144', '214', '224', '234', '244', '314', '324', '334', '344', '414', '424', '434', '444'],\n",
        "               ['413', '314', '313', '414'],\n",
        "               ['331', '341', '431', '441'],\n",
        "               ['422', '423', '424', '432', '433', '434', '442', '443', '444'], \n",
        "               ['244', '234', '243', '233'], \n",
        "               ['144', '134', '143', '133'], \n",
        "               ['122', '111', '121', '112', '221', '212', '211']]"
      ],
      "metadata": {
        "id": "_efsbmOxw1wU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "aAa0f2gLNCQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary for each segment to map them against each customer\n",
        "Description = ['Customers who bought most recently, most often and spend the most',\n",
        "               'Customers who spend the most',\n",
        "               'New Customers who spend the most',\n",
        "               'Active Customers who buy very often but spend less ',\n",
        "               'Customers who have purchased recently',\n",
        "               'Customers who were frequent and good spenders who are becoming very inactive',\n",
        "               'Customers who were frequent and good spenders who are lost contributing to attrition',\n",
        "               'Customers who purchased long ago , less frequent and very little']\n",
        "\n",
        "Marketing = ['No price incentives, New products and Loyalty Programs',\n",
        "                      'Market your most expensive products',\n",
        "                      'Price Incentives',\n",
        "                      'Promote economical cost effective products in daily use',\n",
        "                      'Discounts and promote a variety of product sells',\n",
        "                      'Aggressive Price Incentives',\n",
        "                      'Monitor close communication with customers with constant feedback and rework ',\n",
        "                      'Dont spend too much time to re-acquire',\n",
        "                      ]\n",
        "rfm_segments = pd.DataFrame({'Segment': Segment , 'RFM' : RFM , 'Description': Description, 'Marketing': Marketing})\n",
        "rfm_segments"
      ],
      "metadata": {
        "id": "H2HXRR66w_Oo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recency\n",
        "Recency is about when was the last order of a customer. It means the number of days since a customer made the last purchase. If it’s a case for a website or an app, this could be interpreted as the last visit day or the last login time."
      ],
      "metadata": {
        "id": "74ryExR0xD5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#last date available in our dataset\n",
        "import datetime as dt\n",
        "df_cleaned['InvoiceDate'].max()"
      ],
      "metadata": {
        "id": "R40UYtCty4Ou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets set this date as the today's date for further analysis\n",
        "current_date = dt.date(2011,11,30)\n",
        "current_date"
      ],
      "metadata": {
        "id": "BYVB_IbZy68p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets create a date column for date values only\n",
        "df_cleaned['Purchase_Date'] = df_cleaned.InvoiceDate.dt.date"
      ],
      "metadata": {
        "id": "Gwj6M2Nty9rv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recency = df_cleaned.groupby('CustomerID')['Purchase_Date'].max().reset_index()\n",
        "recency"
      ],
      "metadata": {
        "id": "kI7E_a74zAL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a separate column for this date.\n",
        "recency = recency.assign(Current_Date = current_date)\n",
        "recency"
      ],
      "metadata": {
        "id": "mu-MS0Z1zQ7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the number of days since last purchase\n",
        "recency['Recency'] = recency.Purchase_Date.apply(lambda x: (current_date - x).days)\n",
        "current_date\n"
      ],
      "metadata": {
        "id": "NiDkCrLdzW5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recency.head()"
      ],
      "metadata": {
        "id": "PAVMtHU1zZpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the irrelevant Date columns\n",
        "recency.drop(['Purchase_Date','Current_Date'], axis=1, inplace=True)\n",
        "recency"
      ],
      "metadata": {
        "id": "Tyf8u0unzcGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Frequency\n",
        "Frequency is about the number of purchase in a given period. It could be 3 months, 6 months or 1 year. So we can understand this value as for how often or how many a customer used the product of a company. The bigger the value is, the more engaged the customers are. Could we say them as our VIP? Not necessary. Cause we also have to think about how much they actually paid for each purchase, which means monetary value"
      ],
      "metadata": {
        "id": "BT7oqqG5z_Tq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "frequency = df_cleaned.groupby('CustomerID').InvoiceNo.nunique().reset_index().rename(columns={'InvoiceNo':'Frequency'})\n",
        "frequency.max()"
      ],
      "metadata": {
        "id": "194wDjZv0Deo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Monetary\n",
        "Monetary is the total amount of money a customer spent in that given period. Therefore big spenders will be differentiated with other customers such as MVP or VIP."
      ],
      "metadata": {
        "id": "g41uY0U-0GLs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a separate column for Total Cost of Unit purchased\n",
        "df_cleaned['Total_cost'] = df_cleaned.Quantity * df_cleaned.UnitPrice\n",
        "df_cleaned"
      ],
      "metadata": {
        "id": "ak5Tfb_Y0I0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "monetary = df_cleaned.groupby('CustomerID').Total_cost.sum().reset_index().rename(columns={'Total_cost':'Monetary'})\n",
        "monetary.head()"
      ],
      "metadata": {
        "id": "Ok4wmvk_0MN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now Combine all three to form an aggregated RFM Table"
      ],
      "metadata": {
        "id": "0_H4M3Ge0Oyw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf = recency.merge(frequency, on='CustomerID')\n",
        "rfm_table = rf.merge(monetary, on='CustomerID')"
      ],
      "metadata": {
        "id": "3YSfKYoy0UaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rfm_table.set_index('CustomerID',inplace=True)\n",
        "rfm_table.head()\n"
      ],
      "metadata": {
        "id": "f0_bAzvW0W-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RFM Table integrity Check\n",
        "Let's check whether the RFM table attributes are in conjunction with the original values"
      ],
      "metadata": {
        "id": "g5FWK_aj0Zsz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rfm_table.index[1]"
      ],
      "metadata": {
        "id": "Ck6x7pAi0ck2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch the records corresponding to the first customer id in above table\n",
        "df_cleaned[df_cleaned.CustomerID == rfm_table.index[1]]"
      ],
      "metadata": {
        "id": "TMWWnvTB0e9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the number difference of days from the purchase date in original record is same as shown in rfm table.\n",
        "(current_date - df_cleaned[df_cleaned.CustomerID == rfm_table.index[0]].iloc[0].Purchase_Date).days == rfm_table.iloc[0,0]"
      ],
      "metadata": {
        "id": "ONENpE620iJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Customer segments with RFM Model\n",
        "The simplest way to create customers segments from RFM Model is to use Quantiles. We assign a score from 1 to 4 to Recency, Frequency and Monetary. Four is the best/highest value, and one is the lowest/worst value. A final RFM score is calculated simply by combining individual RFM score numbers."
      ],
      "metadata": {
        "id": "c1qK6uet0k5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RFM Quantiles\n",
        "quantiles = rfm_table.quantile(q=[0.25,0.5,0.75])\n",
        "quantiles"
      ],
      "metadata": {
        "id": "dghtWSON0nkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's convert quartile information into a dictionary so that cutoffs can be picked up.\n",
        "quantiles=quantiles.to_dict()\n",
        "quantiles\n",
        "rfm_table"
      ],
      "metadata": {
        "id": "J6bM_SYf0qBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let us visualize the histogram charts for Recency, Frequency and Monetary\n",
        "plt.hist(rfm_table.Recency, bins = 50, color='c')\n",
        "plt.xlabel('Recency')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KlZeiNco0sNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(rfm_table.Monetary, bins = 50, color='c')\n",
        "plt.xlabel('Monetary')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WpSE7Lgm0ul1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(rfm_table.Frequency, bins = 50, color='c')\n",
        "plt.xlabel('Frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Cfr2S04i0wdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creation of RFM Segments\n",
        "We will create two segmentation classes since, high recency is bad, while high frequency and monetary value is good"
      ],
      "metadata": {
        "id": "bdNOajvR0zgT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Arguments (x = value, p = recency, monetary_value, frequency, d = quantiles dict)\n",
        "def RScore(x,p,d):\n",
        "    if x <= d[p][0.25]:\n",
        "        return 4\n",
        "    elif x <= d[p][0.50]:\n",
        "        return 3\n",
        "    elif x <= d[p][0.75]: \n",
        "        return 2\n",
        "    else:\n",
        "        return 1\n",
        "# Arguments (x = value, p = recency, monetary_value, frequency, k = quantiles dict)\n",
        "def FMScore(x,p,d):\n",
        "    if x <= d[p][0.25]:\n",
        "        return 1\n",
        "    elif x <= d[p][0.50]:\n",
        "        return 2\n",
        "    elif x <= d[p][0.75]: \n",
        "        return 3\n",
        "    else:\n",
        "        return 4\n",
        "rfm_segment = rfm_table.copy()\n",
        "rfm_segment['R_Quartile'] = rfm_segment['Recency'].apply(RScore, args=('Recency',quantiles,))\n",
        "rfm_segment['F_Quartile'] = rfm_segment['Frequency'].apply(FMScore, args=('Frequency',quantiles,))\n",
        "rfm_segment['M_Quartile'] = rfm_segment['Monetary'].apply(FMScore, args=('Monetary',quantiles,))"
      ],
      "metadata": {
        "id": "ZuSCBcQq026c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rfm_segment.head()\n",
        "rfm_segment[rfm_segment.Monetary == rfm_segment.Monetary.max()]\n",
        "rfm_segment"
      ],
      "metadata": {
        "id": "z76Dp8Tm05lE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For analysis it is critical to combine the scores to create a single score. There are few approaches. One approach is to just concatenate the scores to create a 3 digit number between 111 and 444. Here the drawback is too many categories (4x4x4)."
      ],
      "metadata": {
        "id": "dtdtz53808zq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rfm_segment['RFMScore'] = rfm_segment.R_Quartile.map(str) \\\n",
        "                            + rfm_segment.F_Quartile.map(str) \\\n",
        "                            + rfm_segment.M_Quartile.map(str)\n",
        "rfm_segment.head()"
      ],
      "metadata": {
        "id": "nOl5xDGO0_m-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RFM Segment allocation\n",
        "\n",
        "\n",
        "Lets define the customers segment best to our knowledge basis RFM score and assign them to each customer respectively"
      ],
      "metadata": {
        "id": "CmdE3P0L1amE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset the index to create a customer_ID column\n",
        "rfm_segment.reset_index(inplace=True)"
      ],
      "metadata": {
        "id": "PSXzftof1YwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "# Highest frequency as well as monetary value with least recencycy\n",
        "platinum_customers = ['444', '443']\n",
        "print (\"Platinum Customers                     : {}\".format(platinum_customers)) \n",
        "\n",
        "# Get all combinations of [1, 2, 3,4] and length 2 \n",
        "big_spenders_comb =  itertools.product([1, 2, 3,4],repeat = 2)   \n",
        "# Print the obtained combinations \n",
        "big_spenders = []\n",
        "for i in list(big_spenders_comb): \n",
        "    item = (list(i))\n",
        "    item.append(4)\n",
        "    big_spenders.append( (\"\".join(map(str,item))))\n",
        "print (\"Big Spenders                           : {}\".format(big_spenders))\n",
        "\n",
        "#High-spending New Customers – This group consists of those customers in 1-4-1 and 1-4-2.\n",
        "#These are customers who transacted only once, but very recently and they spent a lot\n",
        "\n",
        "high_spend_new_customers = ['413', '314' ,'313','414'] \n",
        "print (\"High Spend New Customers               : {}\".format(high_spend_new_customers))\n",
        "\n",
        "\n",
        "lowest_spending_active_loyal_customers_comb =  itertools.product([ 3,4], repeat = 2)\n",
        "lowest_spending_active_loyal_customers = []\n",
        "for i in list(lowest_spending_active_loyal_customers_comb): \n",
        "    item = (list(i))\n",
        "    item.append(1)\n",
        "    lowest_spending_active_loyal_customers.append( (\"\".join(map(str,item))))\n",
        "print (\"Lowest Spending Active Loyal Customers : {}\".format(lowest_spending_active_loyal_customers))\n",
        "\n",
        "recent_customers_comb =  itertools.product([ 2,3,4], repeat = 2)\n",
        "recent_customers = []\n",
        "for i in list(recent_customers_comb): \n",
        "    item = (list(i))\n",
        "    item.insert(0,4)\n",
        "    recent_customers.append( (\"\".join(map(str,item))))\n",
        "print (\"Recent Customers                       : {}\".format(recent_customers))\n",
        "\n",
        "almost_lost = ['244', '234', '243', '233']        #  Low R - Customer's shopping less often now who used to shop a lot  \n",
        "print (\"Good Customers Almost Lost             : {}\".format(almost_lost)) \n",
        "\n",
        "churned_best_customers = ['144', '134' ,'143','133'] \n",
        "print (\"Churned Best Customers                 : {}\".format(churned_best_customers)) \n",
        "\n",
        "\n",
        "lost_cheap_customers = ['122','111' ,'121','112','221','212' ,'211'] # Customer's shopped long ago but with less frequency and monetary value\n",
        "print (\"Lost Cheap Customers                   : {}\".format(lost_cheap_customers))\n",
        "\n"
      ],
      "metadata": {
        "id": "jiKN_spIHSVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary for each segment to map them against each customer\n",
        "segment_dict = {\n",
        "    'Platinum Customers':platinum_customers,      \n",
        "    'Big Spenders':      big_spenders,\n",
        "    'High Spend New Customers':high_spend_new_customers,\n",
        "    'Lowest-Spending Active Loyal Customers' : lowest_spending_active_loyal_customers ,\n",
        "    'Recent Customers': recent_customers,\n",
        "    'Good Customers Almost Lost':almost_lost,       \n",
        "    'Churned Best Customers':   churned_best_customers, \n",
        "    'Lost Cheap Customers ': lost_cheap_customers, \n",
        "}"
      ],
      "metadata": {
        "id": "Qk6Bwo0OHeCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Allocate segments to each customer as per the RFM score mapping\n",
        "def find_key(value):\n",
        "    for k, v in segment_dict.items():\n",
        "        if value in v:\n",
        "            return k\n",
        "rfm_segment['Segment'] = rfm_segment.RFMScore.map(find_key)\n",
        "\n",
        "# Allocate all remaining customers to others segment category\n",
        "rfm_segment.Segment.fillna('others', inplace=True)\n",
        "rfm_segment.sample(10)"
      ],
      "metadata": {
        "id": "pwrdJS7vHgrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's visualize different customer segments records in general to answers these questions for the retail business.\n",
        "1. Who are my best customers?\n",
        "2. Who are the biggest spenders?\n",
        "3. Which customers are at the verge of churning?\n",
        "4. Who are lost customers that you don’t need to pay much attention to?\n",
        "5. Who are your loyal customers?\n",
        "6. Which customers you must retain?\n",
        "7. Who has the potential to be converted in more profitable customers?\n",
        "8. Which group of customers is most likely to respond to your current campaign?"
      ],
      "metadata": {
        "id": "bq-swNoPH-ud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Best Customers who's recency, frequency as well as monetary attribute is highest.\n",
        "rfm_segment[rfm_segment.RFMScore=='444'].sort_values('Monetary', ascending=False).head()"
      ],
      "metadata": {
        "id": "buVb3a0_IVlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Biggest spenders\n",
        "rfm_segment[rfm_segment.RFMScore=='334'].sort_values('Monetary', ascending=False).head()"
      ],
      "metadata": {
        "id": "eayMK6jzIYG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# customers that you must retain are those whose monetary and frequency was high but recency reduced quite a lot recently\n",
        "rfm_segment[rfm_segment.RFMScore=='244'].sort_values('Monetary', ascending=False).head()"
      ],
      "metadata": {
        "id": "6svG6npZIbC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rfm_segment.to_excel('RFM Segment.xlsx')\n",
        "rfm_segment.Segment.value_counts()\n",
        "rfm_segment.Recency"
      ],
      "metadata": {
        "id": "ax-mTKAqIePg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.cluster.hierarchy as sch\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Selecting only the numeric columns from the dataframe\n",
        "numeric_cols = ['Quantity', 'UnitPrice']\n",
        "X = df_cleaned[numeric_cols].values\n",
        "\n",
        "# Standardize the feature matrix\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Creating a dendrogram to visualize the clusters\n",
        "plt.figure(figsize=(13, 8))\n",
        "dendrogram = sch.dendrogram(sch.linkage(X_scaled, method='ward'))\n",
        "plt.title('Dendrogram')\n",
        "plt.xlabel('Customers')\n",
        "plt.ylabel('Euclidean Distances')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "DwYtKxr8d0qr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting hierarchical clustering\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "hc = AgglomerativeClustering(n_clusters = 2, affinity = 'euclidean', linkage = 'ward')\n",
        "y_hc = hc.fit_predict(X)\n"
      ],
      "metadata": {
        "id": "QGKjm6BigWM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the clusters (two dimensions only)\n",
        "plt.figure(figsize=(13,8))\n",
        "plt.scatter(X[y_hc == 0, 0], X[y_hc == 0, 1], s = 100, c = 'red', label = 'Customer 1')\n",
        "plt.scatter(X[y_hc == 1, 0], X[y_hc == 1, 1], s = 100, c = 'blue', label = 'Customer 2')\n",
        "\n",
        "plt.title('Clusters of Customer')\n",
        "plt.xlabel('RFM')\n",
        "\n",
        "plt.ylabel('Spending Score (1-100)')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GdTdsJCFgag8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#let's check mean values of the cluster for recency, frequnecy and monetary\n",
        "\n",
        "rfm_df.groupby('Cluster').agg({'Recency':'mean',\n",
        "                               'Frequency':'mean',\n",
        "                               'Monetary':'mean'})"
      ],
      "metadata": {
        "id": "gyA9eC03gfqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from prettytable import PrettyTable \n",
        "  \n",
        "# Specify the Column Names while initializing the Table \n",
        "myTable = PrettyTable(['Sr No.',\"Model_Name\",'Data', \"Optimal_Number_of_cluster\"]) \n",
        "  \n",
        "# Add rows \n",
        "myTable.add_row(['1',\"K-Means with silhouette_score \", \"RM\", \"2\"]) \n",
        "myTable.add_row(['2',\"K-Means with Elbow methos  \", \"RM\", \"2\"])\n",
        "myTable.add_row(['3',\"DBSCAN \", \"RM\", \"2\"]) \n",
        "myTable.add_row(['4',\"K-Means with silhouette_score \", \"FM\", \"2\"]) \n",
        "myTable.add_row(['5',\"K-Means with Elbow methos  \", \"FM\", \"2\"])\n",
        "myTable.add_row(['6',\"DBSCAN \", \"FM\", \"2\"])\n",
        "\n",
        "myTable.add_row(['7',\"K-Means with silhouette_score \", \"RFM\", \"2\"]) \n",
        "myTable.add_row(['8',\"K-Means with Elbow methos  \", \"RFM\", \"2\"])\n",
        "myTable.add_row(['9',\"DBSCAN \", \"RFM\", \"3\"])\n",
        "myTable.add_row(['10',\"Hierarchical clustering  \", \"RFM\", \"2\"])\n",
        "\n",
        "print(myTable)"
      ],
      "metadata": {
        "id": "g4Ud-kg0gnUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the conclusion here."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}